
---
title: "PADP8120 Midterm Fall 2015"
author: "Victoria Coxon"
output:
  html_document:
    highlight: pygments
    theme: cerulean
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---


# Midterm Exam

**due by class time (3:35) on October 14, 2015**

## Instructions

A. Fork the [Midterm repository](https://github.com/PADP8120-Fall2015/Midterm) from the course github page to your github account (i.e., just as you would with a homework assignment)

B. In RStudio, create a new project and link it to your newly forked github repo. 

C. Resave the `Midterm_Fall2015.Rmd` file as `Midterm_Fall2015_FirstInitialLastName.Rmd` (e.g., `Midterm_Fall2015_TScott.Rmd`)

D. Complete the midterm within your `Midterm_Fall2015_FirstInitialLastName.Rmd` file. 

E. Make sure your final document renders as an `.html` file. 

F. Please **email** to me all of the materials necessary for another person to run your R Markdown file, including:
  - The R project (`.Rproj`) file
	- The R Markdown document (`.Rmd`) of your analyses
	- An HTML document (`.html`) compiled from your R Markdown document.
	- Any data or other files neede to run the analyses in your R Markdown document.

## Guidelines 

i. This is a take-home, open-book exam. This means that you are welcome to use any resources at your disposal--including textbooks, R help files, and internet resources--EXCEPTING other students (see Guideline #2).  

ii. However, you are expected to work alone, meaning that this exam **must not** be done as a group project or in concert with your classmates (or with the assistant of other SPIA students/faculty). The work you submit is to be yours and yours alone.

iii. Your exam write-up should be clear and legible, with answers clearly indicated and work shown. Your exam must be produced in html (.html) document produced using R Markdown (i.e., an .Rmd file). Submit both your .html file and the .Rmd file used to generate it via github to the course midterm repo. If you are having trouble accomplishing this, please refer to the [guide](http://spia.uga.edu/faculty_pages/tyler.scott/teaching/PADP8120_Fall2015/Homeworks/submitting_homework.shtml). 

iv. The exam will be graded out of 100 points. Each problem is worth a designated number of points (shown below). Partial credit *may* by given for incorrect answers if I am able to see the process by which you went wrong (and thus see what you also did correctly), so it is to your advantage to show your work. 

v. Your exam is **due by class time (3:35) on October 14, 2015**

vi. Please contact me if you have any questions or concerns.

# Problems

You are asked to analyze data from a New York City school voucher experiment that a colleague and I obtained from Mathematica for a paper. 

```{r eval=TRUE}
nyc = read.csv("C:/Users/Victoria/Desktop/PADP 8120/Midterm_Fall2015_VCoxon/input/NYC_SchoolVoucher_Experiment.csv",row.names=1)
str(nyc)
nyc<-read.table("C:/Users/Victoria/Desktop/PADP 8120/Midterm_Fall2015_VCoxon/input/NYC_SchoolVoucher_Experiment.csv", sep=",", header=T)
head(nyc)
str(nyc)
```
There are 2666 observations of 18 variables.

Most of the variables you need should be pretty self explanatory:

| Variable | Description |
|:---|:---|
| `Student_ID` | numerical code for each student |
| `Family_ID` | numerical code for each family |
| `Date.Of.Birth` | student birthday |
| `Female` | binary indicator for female students; 1 = female, 0 = male |
| `Grade`   |  student grade level when starting experiment |
| `Treatment`    |  binary indicator for whether student received voucher; 1 = Voucher Treatment, 0 = No Voucher |
| `Latino`  |   binary indicator for Latino students; 1 = Latino, 0 = Otherwise |
| `Black` |   binary indicator for black students; 1 = Black, 0 = Otherwise |
| `Eldest` |   binary indicator for eldest children; 1 = Eldest, 0 = Otherwise |
| `y0_read_percentile`   |  pre-test reading score percentile |
| `y0_math_percentile`    |  pre-test math score percentile |
| `y1_read_percentile`   |  reading score percentile after 1y |
| `y1_math_percentile`    |  math score percentile after 1y |
| `y2_read_percentile`   |  reading score percentile after 2y |
| `y2_math_percentile`    |  math score percentile after 2y |
| `y3_read_percentile`   |  reading score percentile after 3y |
| `y3_math_percentile`    |  math score percentile after 3y |

1. (6 points) Compute the average reading and math percentiles for each year across all students. 

```{r}
library(knitr)
summary(nyc$y0_read_percentile)
summary(nyc$y0_math_percentile)
summary(nyc$y1_read_percentile)
summary(nyc$y1_math_percentile)
summary(nyc$y2_read_percentile)
summary(nyc$y2_math_percentile)
summary(nyc$y3_read_percentile)
summary(nyc$y3_math_percentile)
```

nyc_Year           |Min. | 1st Qu.|Median | Mean  |3rd Qu. | Max.  | NA's 
 ------------------|-----|--------|-------|-------|--------|-------|---- 
y0_read_percentile |0.00 | 4.00   |16.00  | 22.78 | 34.00  | 99.00 | 815 
y0_math_percentile |0.00 | 1.00   |9.00   | 16.54 | 25.00  | 98.00 | 815
y1_read_percentile |0.00 | 8.00   |20.00  | 26.06 | 40.00  | 97.00 | 586
y1_math_percentile |0.0  | 5.0    |15.0   | 22.3  | 34.0   | 99.0  | 586
y2_read_percentile |0.00 | 8.00   |21.00  | 26.09 | 39.00  | 99.00 | 912 
y2_math_percentile |0.00 | 5.00   |16.00  | 23.56 | 37.00  | 99.00 | 912 
y3_read_percentile |0.00 | 20.00  |33.00  | 32.63 | 44.00  | 95.00 | 865 
y3_math_percentile |0.0  |20.0    |32.0   | 32.5  | 45.0   | 99.0  | 865 

```{r}
library(dplyr)
library(ggplot2)
library(pastecs)

attach(nyc)
scores <- cbind(y0_read_percentile, y0_math_percentile, y1_read_percentile, y1_math_percentile, y2_read_percentile, y2_math_percentile, y3_read_percentile, y3_math_percentile)
stat.desc(scores)

options(scipen=100)
stat.desc(scores)

stat.desc(scores, basic=F)

# Method adapted from http://www.ats.ucla.edu/stat/r/faq/basic_desc.htm 
#The problems with the attach function are known, but the ease with which to obtain the statistics is compelling.
#I compared the means derived from the summary function with those derived from using the cluster of commands beginning with the attach() function to make sure I wasn't missing anything. They are consistent.
```
The mean scores for reading and mathematics are as follows:

Reading                   |      Mathematics
--------------------------|----------------------------------
Reading Year 0 = 22.78    |      Mathematics Year 0 = 16.54 
Reading Year 1 = 26.06    |      Mathematics Year 1 = 22.30
Reading Year 2 = 26.09    |      Mathematics Year 2 = 23.56
Reading Year 3 = 32.63    |      Mathematics Year 3 = 32.50

2. (4 points) Clean the original dataset by removing all students who do not have observed pre-test scores and year 1 scores for reading and for math.

```{r}
stat.desc(scores, desc=F)
# x <- airquality[complete.cases(airquality), ]
clean.nyc <- filter(nyc, y0_read_percentile!='NA', y0_math_percentile!='NA', y1_read_percentile!='NA', y1_math_percentile!='NA')
str(clean.nyc)              
head(clean.nyc)
summary(clean.nyc$y0_read_percentile)
summary(clean.nyc$y0_math_percentile)
summary(clean.nyc$y1_read_percentile)
summary(clean.nyc$y1_math_percentile)
summary(clean.nyc$y2_read_percentile)
summary(clean.nyc$y2_math_percentile)
summary(clean.nyc$y3_read_percentile)
summary(clean.nyc$y3_math_percentile)
```
Clean_Year         |Min. | 1st Qu.|Median | Mean  |3rd Qu. | Max.  | NA's 
 ------------------|-----|--------|-------|-------|--------|-------|---- 
y0_read_percentile |0.00 | 4.00   |16.00  | 22.57 | 34.00  | 99.00 | 
y0_math_percentile |0.00 | 2.00   |9.00   | 16.42 | 25.00  | 98.00 | 
y1_read_percentile |0.00 | 7.00   |18.00  | 24.63 | 37.50  | 97.00 | 
y1_math_percentile |0.00 | 5.00   |15.00  | 22.24 | 36.00  | 99.00 | 
y2_read_percentile |0.00 | 8.00   |20.00  | 25.26 | 38.00  | 98.00 | 343 
y2_math_percentile |0.00 | 5.00   |16.00  | 24.24 | 37.00  | 99.00 | 343 
y3_read_percentile |0.00 | 21.00  |34.00  | 33.54 | 46.00  | 95.00 | 333 
y3_math_percentile |0.00 | 21.00  |32.00  | 33.23 | 45.00  | 99.00 | 333 


```{r}
attach(clean.nyc)
clean.scores <- cbind(y0_read_percentile, y0_math_percentile, y1_read_percentile, y1_math_percentile, y2_read_percentile, y2_math_percentile, y3_read_percentile, y3_math_percentile)
stat.desc(scores)

options(scipen=100)
stat.desc(clean.scores)

stat.desc(clean.scores, basic=F)
```
I cleaned the data and then checked to see if there was a dfference in the summary statistics based on the scores from the cleaned data. There is a difference, as would be expected when missing observations are dropped. Dropping observations with missing values effectively lowered the sample size from 2666 observations to 1455 observations.We might want to be a little more cautious in any inferences we make from this data.
Reading Year 0 = 22.78; Clean = 22.57          Mathematics Year 0 = 16.54; Clean = 16.42  
Reading Year 1 = 26.06; Clean = 24.63          Mathematics Year 1 = 22.30; Clean = 23.24
Reading Year 2 = 26.09; Clean = 25.26          Mathematics Year 2 = 23.56; Clean = 24.24 
Reading Year 3 = 32.63; Clean = 33.54          Mathematics Year 3 = 32.50; Clean = 33.23

3. (8 points) Make a figure (either a density plot of a histgram format) comparing the distributions of percentile reading scores for students who did and did not receive a voucher. Do your best to make the figure "publishable" by giving it clear axis titles, legend labels, and other aesthetic improvements. 

```{r}
no.vouch <- filter(clean.nyc,Treatment=="1")
vouch <- filter(clean.nyc,Treatment=="0")
qplot(no.vouch$y0_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 0 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("blue"))
qplot(vouch$y0_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 0 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("darkgreen"))
qplot(no.vouch$y1_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 1 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("blue"))
qplot(vouch$y1_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 1 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("darkgreen"))
qplot(no.vouch$y2_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 2 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("blue"))
qplot(vouch$y2_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 2 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("darkgreen"))
qplot(no.vouch$y3_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 3 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("blue"))
qplot(vouch$y3_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 3 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("darkgreen"))
```
These were done with the cleaned data.  Now I want to look at the original "dirty" data to seee if the right skew is still there. If so, I have concerns about applying the normal model.
```{r}
nyc.no.vouch <- filter(nyc,Treatment=="1")
nyc.vouch <- filter(nyc,Treatment=="0")
qplot(nyc.no.vouch$y0_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 0 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("lightblue"))
qplot(nyc.vouch$y0_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 0 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("lightgreen"))
qplot(nyc.no.vouch$y1_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 1 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("lightblue"))
qplot(nyc.vouch$y1_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 1 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("lightgreen"))
qplot(nyc.no.vouch$y2_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 2 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("lightblue"))
qplot(nyc.vouch$y2_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 2 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("lightgreen"))
qplot(nyc.no.vouch$y3_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 3 Reading Scores - No Voucher", xlab = "Reading Percentile Scores", fill=I("lightblue"))
qplot(nyc.vouch$y3_read_percentile,geom = "histogram", binwidth = 1.0, main = "Year 3 Reading Scores - Voucher", xlab = "Reading Percentile Scores", fill=I("lightgreen"))

```


4. (4 points) In no more than 2-3 sentences, describe what your visualization in 1.C indicates about these data. Do the treatment and control groups appear to be similar (in terms of reading pre-test scores)? Why or why not? 
```{r}
summary(vouch$y0_read_percentile)
summary(no.vouch$y0_read_percentile)
```
Looking at both histograms and summary statstics for pre-test reading scores of both the dirty and clean data, it appears that the groups are similar despite slightly higher dispersion in the no.voucher set. Regardlesss, they appear to be a good match for isolating the effect of the voucher using test scores (even though we are not necessarily discussing causality at this point). Additionally, this comparability appears to hold for years 1, 2, and 3.  

5. (4 points) Typically, policy experiments such as the NYC school voucher lottery experiment are indended to produce statistical evidence that can then be generalized to broader policy applications (i.e., how might vouchers influence educational outcomes for other students who weren't part of the original experiment). Based on the reading pre-test scores that you visualized in C, describe any concerns you might have about our ability to generalize the results of this experiment to ALL elementary school students in the United States in no more than 2-3 sentences. 

I have concerns about the situation of the mean. Test scores are usually normally distributed; this is not the case in this sample. There is an obvious and consistent right skew and the mean is nowhere near the 50th %ile leading me to conclude that this sample is not a representative sample of ALL elementary school students. 

6. (4 points) What is the probability that a randomly selected student in the dataset received a voucher and scored above the 50th percentile on the math pre-test?
```{r}
table(nyc$Treatment,nyc$y0_math_percentile > 50)
(896+67) / (824+64+896+67)
# P(voucher) = 0.5202593
(64+67)/(624+50+896+67)
# P(y0_math_percentile >50) = 0.08002443
67/(624+50+896+67) 
0.5202593 * 0.04092853
```
The probability that a randomly selected student in the dataset received a voucher and scored above the 50th percentile on the math pre-test is 0.04092853. 

7. (4 points) What is the probability that a randomly selected student who received a voucher scored above the 50th percentile on the math pre-test?
```{r}
table(clean.nyc$Treatment,clean.nyc$y0_math_percentile > 50)
53/(728+53)
```
The probability that a randomly selected student who received a voucher scored above the 50th percentile on the math pre-test is 0.06786172.

8. (4 points) We are interested in studying whether the change in math scores after one year is different for students who receive a voucher versus those who do not. Write the null and alternative hypotheses in words and then using symbols.

Null: After one year, there is no average difference in math scores between studentes who receive vouchers and those who do not.
$H_0$: [$\mu_{math.vouch.y1} - \mu_{math.vouch.y0}] - [\mu_{math.no.vouch.y1} - \mu_{math.no.vouch}] = 0$
Alternative: After one year, there is an average difference in math scores between studentes who receive vouchers and those who do not.
$H_A$: [$\mu_{math.vouch.y1} - \mu_{math.vouch.y0}] - [\mu_{math.no.vouch.y1} - \mu_{math.no.vouch}] != 0$ 

9. (6 points) Evaluate the hypothesis from 1.H at a significance level of $\alpha = 0.05$ and state your conclusion in 1-2 sentences. 

10. (2 points) Would your conclusion change at $\alpha = 0.10$? Why or why not?

11. (6 points) There are four assumptions that hold for the test above to be valid: independence within groups, independence between groups, sample sizes both above 30, and symmetric distributions. Are you satisfied that each of these are upheld? Is there anything in particular that you might be concerned about? 

12. (4 points) Next, you are asked to evaluate whether students in the treatment group differ in terms of their reading ability from the pre-test to the year 1 post-test. Write out the null and alternative hypotheses in words and then in symbols. 

13. (6 points) Conduct a t-test to evaluate this hypothesis at $\alpha = 0.01$ significance level. Interpret your results in 1-2 complete sentences.  
 
14. (4 points) Explain how you could evaluate the hypothesis above using a confidence interval instead of a p-value.

15. (2 points) Interpret the confidence interval you estimated for question N in a sentence.

16. (8 points) Perform a test of whether the average math pre-test score differs by grade level at $\alpha = 0.10$ significance level. Write out your hypothesis in symbols, perform the test, and then report your findings in a complete sentence.

17. (8 points) The NYC School District considers students who perform at or above the 50th percentile to be "passing". At a 99% significance level, is the proportion of all students in the sample (voucher and no-voucher) that are considered to be passing in math significantly different between the pre-test and year 1 post-test?

18. (6 points) Make a boxplot that compares the distribution of math scores between the pre-test and year 1 post-test for all students. Do your best to make the figure "publishable" by giving it clear axis titles, legend labels, and other aesthetic improvements. In no more than 2-3 sentences, describe how your figure supports (or does not support) your finding from question 17.

19. (4 points) Consider the correlation between math percentile and reading percentile for year 1 test scores. Compute Pearson’s `r` between these two variables. Is the correlation higher or lower than you would have expected? 

20. (6 points) You might argue that percentiles are actually rank-order data and not ratio-level data. For rank-order data, a more appropriate correlation measure is Spearman’s $\rho$, which is a rank correlation. Compute $\rho$ and contrast the results from $r$ (for question S above). Why do the results differ?

